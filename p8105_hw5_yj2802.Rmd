---
title: "p8105_hw5_yj2802"
author: "Yizhen Jia"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r}
library(tidyverse)
library(rvest)
library(broom)
library(knitr)
```

## Problem 1

```{r}
birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  return(any(duplicated(birthdays)))
}

simulate_probabilities = function(min_size, max_size, simulations) {
  results = tibble(
    group_size = min_size:max_size,
    probability = map_dbl(min_size:max_size, ~{
      mean(replicate(simulations, birthday(.x)))
    })
  )
  return(results)
}

min_group_size = 2
max_group_size = 50
simulations = 10000
results = simulate_probabilities(min_group_size, max_group_size, simulations)

ggplot(results, aes(x = group_size, y = probability)) +
  geom_line() +
  labs(
    title = "Probability of Shared Birthdays in a Group",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()
```

As the group size increases, the probability of at least two people sharing a birthday rises rapidly. The probability crosses 50% around group size 23, illustrating the birthday paradox.

## Problem 2

```{r}
set.seed(123)

n = 30
sigma = 5
alpha = 0.05
sims = 5000
mu_vals = c(0, 1, 2, 3, 4, 5, 6)

power = c()
mean_all = c()
mean_rej = c()

for (mu in mu_vals) {
  res = replicate(sims, {
    data = rnorm(n, mean = mu, sd = sigma)
    t_test = t.test(data, mu = 0) |> tidy()
    list(est = t_test$estimate, p = t_test$p.value)
  }, simplify = FALSE) |> bind_rows()
  power = c(power, mean(res$p < alpha))
  mean_all = c(mean_all, mean(res$est))
  mean_rej = c(mean_rej, mean(res$est[res$p < alpha]))
}

p2 = tibble(mu = mu_vals, power = power, mean_all = mean_all, mean_rej = mean_rej)

ggplot(p2, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(x = "True value of μ", y = "Proportion of Null Rejections (Power)", title = "Power vs. Effect Size (True μ)") +
  theme_minimal()
```

As the effect size increases, the power of the test rises rapidly, indicating a higher likelihood of detecting a true effect. The relationship is non-linear, with power increasing quickly for moderate effect sizes and plateauing as the effect size becomes large.

```{r}
ggplot(p2, aes(x = mu)) +
  geom_line(aes(y = mean_all, color = "Average μ^ (All samples)")) +
  geom_point(aes(y = mean_all, color = "Average μ^ (All samples)")) +
  geom_line(aes(y = mean_rej, color = "Average μ^ (Rejected samples)"), linetype = "dashed") +
  geom_point(aes(y = mean_rej, color = "Average μ^ (Rejected samples)")) +
  labs(x = "True value of μ", y = "Average estimate of μ^", title = "Average μ^ vs. True μ") +
  scale_color_manual(values = c("Average μ^ (All samples)" = "blue", "Average μ^ (Rejected samples)" = "red")) +
  theme_minimal()
```

Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

No, the sample average of 𝜇̂ across tests for which the null is rejected is not exactly equal to the true value of μ, especially for smaller μ values. This is because samples that lead to rejection of the null tend to have more extreme values, introducing a positive bias in the average estimate.

## Problem 3

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides = read_csv(url)
```

The raw data homicides have `r nrow(homicides)` ovservations and `r ncol(homicides)` variables. The included variables are: `r colnames(homicides)`.

```{r}
homicides = read_csv(url) |> 
  mutate(city_state = paste(city, state, sep = ", "))

city_summary = homicides |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

baltimore_summary = homicides |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

baltimore_test = prop.test(
  baltimore_summary$unsolved_homicides, 
  baltimore_summary$total_homicides
) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_table = baltimore_test |> 
  mutate(
    city = "Baltimore, MD",
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  select(city, estimate, conf.low, conf.high)

knitr::kable(
  baltimore_table, 
  caption = "Proportion of Unsolved Homicides in Baltimore, MD"
)

city_results = city_summary |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    tidy_results = map(prop_test, tidy)
  ) |> 
  unnest(cols = tidy_results) |> 
  select(city_state, estimate, conf.low, conf.high)

city_table = city_results |> 
  arrange(city_state) |>
  mutate(
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  rename(
    `City, State` = city_state,
    `Proportion` = estimate,
    `Lower CI` = conf.low,
    `Upper CI` = conf.high
  )

knitr::kable(
  city_table, 
  caption = "Proportion of Unsolved Homicides Across U.S. Cities"
)


# Create a ggplot
ggplot(city_results |> mutate(city_state = reorder(city_state, estimate)), aes(x = city_state, y = estimate)) +
  geom_point(color = "blue", size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "darkgray") +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides Across U.S. Cities",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides",
    caption = "Error bars represent 95% confidence intervals"
  ) +
  theme_minimal()
```
