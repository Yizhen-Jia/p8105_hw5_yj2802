---
title: "p8105_hw5_yj2802"
author: "Yizhen Jia"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r}
library(tidyverse)
library(rvest)
library(broom)
library(knitr)
```

## Problem 1

```{r}
birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  return(any(duplicated(birthdays)))
}

simulate_probabilities = function(min_size, max_size, simulations) {
  results = tibble(
    group_size = min_size:max_size,
    probability = map_dbl(min_size:max_size, ~{
      mean(replicate(simulations, birthday(.x)))
    })
  )
  return(results)
}

min_group_size = 2
max_group_size = 50
simulations = 10000
results = simulate_probabilities(min_group_size, max_group_size, simulations)

ggplot(results, aes(x = group_size, y = probability)) +
  geom_line() +
  labs(
    title = "Probability of Shared Birthdays in a Group",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()
```

As the group size increases, the probability of at least two people sharing a birthday rises rapidly. The probability crosses 50% around group size 23, illustrating the birthday paradox.

## Problem 2

```{r}
set.seed(123)

n = 30
sigma = 5
alpha = 0.05
sims = 5000
mu_vals = c(0, 1, 2, 3, 4, 5, 6)

power = c()
mean_all = c()
mean_rej = c()

for (mu in mu_vals) {
  res = replicate(sims, {
    data = rnorm(n, mean = mu, sd = sigma)
    t_test = t.test(data, mu = 0) |> tidy()
    list(est = t_test$estimate, p = t_test$p.value)
  }, simplify = FALSE) |> bind_rows()
  power = c(power, mean(res$p < alpha))
  mean_all = c(mean_all, mean(res$est))
  mean_rej = c(mean_rej, mean(res$est[res$p < alpha]))
}

p2 = tibble(mu = mu_vals, power = power, mean_all = mean_all, mean_rej = mean_rej)

ggplot(p2, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(x = "True value of Î¼", y = "Proportion of Null Rejections (Power)", title = "Power vs. Effect Size (True Î¼)") +
  theme_minimal()
```

As the effect size increases, the power of the test rises rapidly, indicating a higher likelihood of detecting a true effect. The relationship is non-linear, with power increasing quickly for moderate effect sizes and plateauing as the effect size becomes large.

```{r}
ggplot(p2, aes(x = mu)) +
  geom_line(aes(y = mean_all, color = "Average Î¼^ (All samples)")) +
  geom_point(aes(y = mean_all, color = "Average Î¼^ (All samples)")) +
  geom_line(aes(y = mean_rej, color = "Average Î¼^ (Rejected samples)"), linetype = "dashed") +
  geom_point(aes(y = mean_rej, color = "Average Î¼^ (Rejected samples)")) +
  labs(x = "True value of Î¼", y = "Average estimate of Î¼^", title = "Average Î¼^ vs. True Î¼") +
  scale_color_manual(values = c("Average Î¼^ (All samples)" = "blue", "Average Î¼^ (Rejected samples)" = "red")) +
  theme_minimal()
```

Is the sample average of Î¼^ across tests for which the null is rejected approximately equal to the true value of Î¼? Why or why not?

No, the sample average of ðœ‡Ì‚ across tests for which the null is rejected is not exactly equal to the true value of Î¼, especially for smaller Î¼ values. This is because samples that lead to rejection of the null tend to have more extreme values, introducing a positive bias in the average estimate.

## Problem 3

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides = read_csv(url)
```

The raw data homicides have `r nrow(homicides)` ovservations and `r ncol(homicides)` variables. The included variables are: `r colnames(homicides)`.

```{r}
homicides = read_csv(url) |> 
  mutate(city_state = paste(city, state, sep = ", "))

city_summary = homicides |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

baltimore_summary = homicides |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

baltimore_test = prop.test(
  baltimore_summary$unsolved_homicides, 
  baltimore_summary$total_homicides
) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_table = baltimore_test |> 
  mutate(
    city = "Baltimore, MD",
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  select(city, estimate, conf.low, conf.high)

knitr::kable(
  baltimore_table, 
  caption = "Proportion of Unsolved Homicides in Baltimore, MD"
)

city_results = city_summary |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    tidy_results = map(prop_test, tidy)
  ) |> 
  unnest(cols = tidy_results) |> 
  select(city_state, estimate, conf.low, conf.high)

city_table = city_results |> 
  arrange(city_state) |>
  mutate(
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  rename(
    `City, State` = city_state,
    `Proportion` = estimate,
    `Lower CI` = conf.low,
    `Upper CI` = conf.high
  )

knitr::kable(
  city_table, 
  caption = "Proportion of Unsolved Homicides Across U.S. Cities"
)


# Create a ggplot
ggplot(city_results |> mutate(city_state = reorder(city_state, estimate)), aes(x = city_state, y = estimate)) +
  geom_point(color = "blue", size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "darkgray") +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides Across U.S. Cities",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides",
    caption = "Error bars represent 95% confidence intervals"
  ) +
  theme_minimal()
```
