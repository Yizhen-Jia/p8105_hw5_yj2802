p8105_hw5_yj2802
================
Yizhen Jia
2024-11-16

``` r
library(tidyverse)
```

    ## â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€
    ## âœ” dplyr     1.1.4     âœ” readr     2.1.5
    ## âœ” forcats   1.0.0     âœ” stringr   1.5.1
    ## âœ” ggplot2   3.5.1     âœ” tibble    3.2.1
    ## âœ” lubridate 1.9.3     âœ” tidyr     1.3.1
    ## âœ” purrr     1.0.2     
    ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
    ## âœ– dplyr::filter() masks stats::filter()
    ## âœ– dplyr::lag()    masks stats::lag()
    ## â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(rvest)
```

    ## 
    ## Attaching package: 'rvest'
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

``` r
library(broom)
library(knitr)
```

## Problem 1

``` r
birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  return(any(duplicated(birthdays)))
}

simulate_probabilities = function(min_size, max_size, simulations) {
  results = tibble(
    group_size = min_size:max_size,
    probability = map_dbl(min_size:max_size, ~{
      mean(replicate(simulations, birthday(.x)))
    })
  )
  return(results)
}

min_group_size = 2
max_group_size = 50
simulations = 10000
results = simulate_probabilities(min_group_size, max_group_size, simulations)

ggplot(results, aes(x = group_size, y = probability)) +
  geom_line() +
  labs(
    title = "Probability of Shared Birthdays in a Group",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()
```

![](p8105_hw5_yj2802_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

As the group size increases, the probability of at least two people
sharing a birthday rises rapidly. The probability crosses 50% around
group size 23, illustrating the birthday paradox.

## Problem 2

``` r
set.seed(123)

n = 30
sigma = 5
alpha = 0.05
sims = 5000
mu_vals = c(0, 1, 2, 3, 4, 5, 6)

power = c()
mean_all = c()
mean_rej = c()

for (mu in mu_vals) {
  res = replicate(sims, {
    data = rnorm(n, mean = mu, sd = sigma)
    t_test = t.test(data, mu = 0) |> tidy()
    list(est = t_test$estimate, p = t_test$p.value)
  }, simplify = FALSE) |> bind_rows()
  power = c(power, mean(res$p < alpha))
  mean_all = c(mean_all, mean(res$est))
  mean_rej = c(mean_rej, mean(res$est[res$p < alpha]))
}

p2 = tibble(mu = mu_vals, power = power, mean_all = mean_all, mean_rej = mean_rej)

ggplot(p2, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(x = "True value of Î¼", y = "Proportion of Null Rejections (Power)", title = "Power vs. Effect Size (True Î¼)") +
  theme_minimal()
```

![](p8105_hw5_yj2802_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

As the effect size increases, the power of the test rises rapidly,
indicating a higher likelihood of detecting a true effect. The
relationship is non-linear, with power increasing quickly for moderate
effect sizes and plateauing as the effect size becomes large.

``` r
ggplot(p2, aes(x = mu)) +
  geom_line(aes(y = mean_all, color = "Average Î¼^ (All samples)")) +
  geom_point(aes(y = mean_all, color = "Average Î¼^ (All samples)")) +
  geom_line(aes(y = mean_rej, color = "Average Î¼^ (Rejected samples)"), linetype = "dashed") +
  geom_point(aes(y = mean_rej, color = "Average Î¼^ (Rejected samples)")) +
  labs(x = "True value of Î¼", y = "Average estimate of Î¼^", title = "Average Î¼^ vs. True Î¼") +
  scale_color_manual(values = c("Average Î¼^ (All samples)" = "blue", "Average Î¼^ (Rejected samples)" = "red")) +
  theme_minimal()
```

![](p8105_hw5_yj2802_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Is the sample average of Î¼^ across tests for which the null is rejected
approximately equal to the true value of Î¼? Why or why not?

No, the sample average of ðœ‡Ì‚ across tests for which the null is rejected
is not exactly equal to the true value of Î¼, especially for smaller Î¼
values. This is because samples that lead to rejection of the null tend
to have more extreme values, introducing a positive bias in the average
estimate.

## Problem 3

``` r
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides = read_csv(url)
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The raw data homicides have 52179 ovservations and 12 variables. The
included variables are: uid, reported_date, victim_last, victim_first,
victim_race, victim_age, victim_sex, city, state, lat, lon, disposition.

``` r
homicides = read_csv(url) |> 
  mutate(city_state = paste(city, state, sep = ", "))
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
city_summary = homicides |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

baltimore_summary = homicides |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

baltimore_test = prop.test(
  baltimore_summary$unsolved_homicides, 
  baltimore_summary$total_homicides
) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_table = baltimore_test |> 
  mutate(
    city = "Baltimore, MD",
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  select(city, estimate, conf.low, conf.high)

knitr::kable(
  baltimore_table, 
  caption = "Proportion of Unsolved Homicides in Baltimore, MD"
)
```

| city          | estimate | conf.low | conf.high |
|:--------------|---------:|---------:|----------:|
| Baltimore, MD |    0.646 |    0.628 |     0.663 |

Proportion of Unsolved Homicides in Baltimore, MD

``` r
city_results = city_summary |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    tidy_results = map(prop_test, tidy)
  ) |> 
  unnest(cols = tidy_results) |> 
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## â„¹ In argument: `prop_test = map2(unsolved_homicides, total_homicides,
    ##   ~prop.test(.x, .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
city_table = city_results |> 
  arrange(city_state) |>
  mutate(
    estimate = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  rename(
    `City, State` = city_state,
    `Proportion` = estimate,
    `Lower CI` = conf.low,
    `Upper CI` = conf.high
  )

knitr::kable(
  city_table, 
  caption = "Proportion of Unsolved Homicides Across U.S. Cities"
)
```

| City, State        | Proportion | Lower CI | Upper CI |
|:-------------------|-----------:|---------:|---------:|
| Albuquerque, NM    |      0.386 |    0.337 |    0.438 |
| Atlanta, GA        |      0.383 |    0.353 |    0.415 |
| Baltimore, MD      |      0.646 |    0.628 |    0.663 |
| Baton Rouge, LA    |      0.462 |    0.414 |    0.511 |
| Birmingham, AL     |      0.434 |    0.399 |    0.469 |
| Boston, MA         |      0.505 |    0.465 |    0.545 |
| Buffalo, NY        |      0.612 |    0.569 |    0.654 |
| Charlotte, NC      |      0.300 |    0.266 |    0.336 |
| Chicago, IL        |      0.736 |    0.724 |    0.747 |
| Cincinnati, OH     |      0.445 |    0.408 |    0.483 |
| Columbus, OH       |      0.530 |    0.500 |    0.560 |
| Dallas, TX         |      0.481 |    0.456 |    0.506 |
| Denver, CO         |      0.542 |    0.485 |    0.598 |
| Detroit, MI        |      0.588 |    0.569 |    0.608 |
| Durham, NC         |      0.366 |    0.310 |    0.426 |
| Fort Worth, TX     |      0.464 |    0.422 |    0.507 |
| Fresno, CA         |      0.347 |    0.305 |    0.391 |
| Houston, TX        |      0.507 |    0.489 |    0.526 |
| Indianapolis, IN   |      0.449 |    0.422 |    0.477 |
| Jacksonville, FL   |      0.511 |    0.482 |    0.540 |
| Kansas City, MO    |      0.408 |    0.380 |    0.437 |
| Las Vegas, NV      |      0.414 |    0.388 |    0.441 |
| Long Beach, CA     |      0.413 |    0.363 |    0.464 |
| Los Angeles, CA    |      0.490 |    0.469 |    0.511 |
| Louisville, KY     |      0.453 |    0.412 |    0.495 |
| Memphis, TN        |      0.319 |    0.296 |    0.343 |
| Miami, FL          |      0.605 |    0.569 |    0.640 |
| Milwaukee, wI      |      0.361 |    0.333 |    0.391 |
| Minneapolis, MN    |      0.511 |    0.459 |    0.563 |
| Nashville, TN      |      0.362 |    0.329 |    0.398 |
| New Orleans, LA    |      0.649 |    0.623 |    0.673 |
| New York, NY       |      0.388 |    0.349 |    0.427 |
| Oakland, CA        |      0.536 |    0.504 |    0.569 |
| Oklahoma City, OK  |      0.485 |    0.447 |    0.524 |
| Omaha, NE          |      0.413 |    0.365 |    0.463 |
| Philadelphia, PA   |      0.448 |    0.430 |    0.466 |
| Phoenix, AZ        |      0.551 |    0.518 |    0.584 |
| Pittsburgh, PA     |      0.534 |    0.494 |    0.573 |
| Richmond, VA       |      0.263 |    0.223 |    0.308 |
| Sacramento, CA     |      0.370 |    0.321 |    0.421 |
| San Antonio, TX    |      0.429 |    0.395 |    0.463 |
| San Bernardino, CA |      0.618 |    0.558 |    0.675 |
| San Diego, CA      |      0.380 |    0.335 |    0.426 |
| San Francisco, CA  |      0.507 |    0.468 |    0.545 |
| Savannah, GA       |      0.467 |    0.404 |    0.532 |
| St.Â Louis, MO      |      0.540 |    0.515 |    0.564 |
| Stockton, CA       |      0.599 |    0.552 |    0.645 |
| Tampa, FL          |      0.457 |    0.388 |    0.527 |
| Tulsa, AL          |      0.000 |    0.000 |    0.945 |
| Tulsa, OK          |      0.331 |    0.293 |    0.371 |
| Washington, DC     |      0.438 |    0.411 |    0.465 |

Proportion of Unsolved Homicides Across U.S. Cities

``` r
# Create a ggplot
ggplot(city_results |> mutate(city_state = reorder(city_state, estimate)), aes(x = city_state, y = estimate)) +
  geom_point(color = "blue", size = 3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "darkgray") +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides Across U.S. Cities",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides",
    caption = "Error bars represent 95% confidence intervals"
  ) +
  theme_minimal()
```

![](p8105_hw5_yj2802_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->
